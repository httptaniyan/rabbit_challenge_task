{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 入力層～中間層\n",
    "入力層の機能は下記<br>\n",
    "・深層学習モデルが何かを予測するための**特徴量(説明変数)**を受ける機能<br>\n",
    "・特徴量を受け取った後、中間層へ伝搬させるため(入力するため)の機能<br>\n",
    "　中間層へ伝搬されるものとしては、特徴量${\\omega}_i$と$x_i$の**線型結合**で表すことができる。<br>\n",
    "　ただし、$b = x_0 = 1$として、以後$b$のことを**バイアス**と呼ぶ。<br><br>\n",
    "中間層の機能は下記<br>\n",
    "・入力層の総入力(線型結合)を受け取る<br>\n",
    "・受け取った総入力を活性化関数に入力<br>\n",
    "・活性化関数の出力を次の層に伝搬させる(中間層や出力層)<br>\n",
    "\n",
    "イメージと数式は下記である。<br>\n",
    "![入力層と出力層のimg](写真/入力層と中間層のimg.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 活性化関数\n",
    "### 活性化関数とは\n",
    "・ニューラルネットワークにおいて、次の層への出力の大きさを決める非線形の関数。 <br>　\n",
    "・入力値の値によって、次の層への信号のON/OFFや強弱を定める働きをもつ。<br>\n",
    "\n",
    "### 活性化関数の例\n",
    "**ステップ関数**<br>　\n",
    "閾値を超えたら発火する関数。出力は常に０か１。<br>　\n",
    "パーセプトロン（ニューラルネットワークの前身）では使われていたが深層学習になってからは使われていない。<br><br>　\n",
    "ステップ関数の課題<br>\n",
    "　０〜１間の間を表現できず、線形分離可能なものしか学習できなかった。<br>\n",
    "　人間に模倣したものを使ったはずなのに、微細な表現ができなかった。<br>\n",
    "　さらに、０〜１の値が表現できなかったので、線形分離可能な問題しか学習できなかった。<br>\n",
    "\n",
    "**シグモイド関数**<br>\n",
    "　ステップ関数では人間のような微細な表現を表現するために、**シグモイド関数**が登場する。<br>\n",
    "　０〜１の間を緩やかに変化するような関数ができて、微細な表現ができるようになった。<br>\n",
    "　ニューラルネットワーク普及の きっかけとなった。<br>\n",
    "\n",
    "　シグモイド関数の課題<br>\n",
    "　　入力が大きな値(０から離れると)では出力の変化が微小なため、下記の課題がある。<br>\n",
    "　　　**・スパース化ができない(0にならない)**<br>\n",
    "　　　⇨微小な値を0にしないと有限な計算リソースの無駄である。<br><br>\n",
    "　　　**・勾配消失問題**<br>\n",
    "　　　⇨シグモイドの1次導関数の最大値は0.25。多層になるほど誤差逆伝播による学習効果は薄くなってしまう。<br>\n",
    "　　　参考になるスライドを添付する(⇨https://www.slideshare.net/YoshihiroTakahashi6/ss-81043296)<br>\n",
    " ![活性化関数の1次導関数関するimg](写真/grad_info.png)\n",
    "\n",
    "**RELU関数**<br>　\n",
    "勾配消失問題やスパース化できない問題に対処するためにRELU関数が出た。<br>　\n",
    "勾配消失問題とスパース化に貢献することで良い結果をもたらしている。<br>\n",
    "\n",
    "RELU関数のメリットとしてはまとめると下記<br>\n",
    "⇨小さい値を与え続けられない(スパース化)、かつ大きい値を返すことができる(勾配消失問題に対応)<br>　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 出力層\n",
    "### 出力層の機能\n",
    "中間層までの計算は人間が見てもわかりにくい数値として出るが、出力層では人間が視覚的にわかる理論値で出てくる。<br>\n",
    "例えば、分類問題であったら確率値が出る。<br><br>\n",
    "学習する際には、この確率値(予測値)と正解データとの誤差を用いて誤差が無くなるように学習をする。<br>\n",
    "しかし、誤差には様々な指標がある。そこで誤差の指標として用いられるのが誤差関数である。<br>\n",
    "深層学習ではこの誤差関数をもとに誤差が最小になるように学習をするのが目標。<br>\n",
    "### 誤差関数\n",
    "・二乗誤差(⇨分類問題では使わないが理解しやすい誤差関数を選定)<br>\n",
    "　$E_n(w) = {\\frac{1}{2}}{\\Sigma{_{i=1}^{I}}}(y_i - d_i)^2$<br>\n",
    "・交差エントロピー(⇨分類問題で使用)<br>\n",
    "　$E_n(w) = -{\\Sigma{_{i=1}^{I}}}{d_i}{\\log}{y_i}$<br>\n",
    "### 出力層と中間層で使用される活性化関数の目的の違い\n",
    "**値の強弱**<br>\n",
    "・中間層: 閾値の前後で信号の強弱を調整<br>\n",
    "・出力層: 信号の大きさ(比率)はそのままに変換<br>\n",
    "**確率出力**<br>\n",
    "・分類問題の場合、出力層の出力は0~1の範囲に限定し、総和を1とする必要がある<br><br>\n",
    "出力層と中間層で利用される活性化関数が異なることを覚えておくこと <br>\n",
    "\n",
    "### 出力層で使用される活性化関数の例\n",
    "分類問題<br>\n",
    "　・ソフトマックス関数(他クラス分類に使用)<br>\n",
    "　　⇨誤差関数は交差エントロピー<br><br>\n",
    "　・シグモイド関数(2値分類に使用)<br>\n",
    "　　⇨誤差関数は交差エントロピー<br><br>\n",
    "回帰問題<br>\n",
    "　・恒等関数<br>\n",
    "　　⇨誤差関数は平均誤差など<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 勾配降下法\n",
    "上述したように、深層学習の目的は学習を通して誤差を最小ことである。<br>\n",
    "⇨ 誤差$E(w)$を最小にするパラメータ$w$と$b$を探すことである。<br>\n",
    "\n",
    "最小にするパラメータを探すために使われる手法が**勾配降下法**である。<br>\n",
    "１回のパラメータ更新に訓練データすべてを利用する。<br>\n",
    "そのため、学習データが多いと計算コストがとても大きくなる。また、冗長な計算もしているかもしれない。<br>\n",
    "\n",
    "**勾配降下法**：　$w^{(t+1)} = w^{(t)} - {\\varepsilon}{\\nabla}E$<br>\n",
    "　　　　　　　${\\nabla}E = {\\frac{\\partial{E}}{\\partial{w}}} = {[{\\frac{\\partial{E}}{\\partial{w_1}}}, {\\frac{\\partial{E}}{\\partial{w_2}}}, …, {\\frac{\\partial{E}}{\\partial{w_M}}}]}$<br>\n",
    "${\\varepsilon}$は、学習率である。<br>\n",
    "学習率の取り扱いには注意が必要。<br>\n",
    "### 学習率について\n",
    "学習率を大きく取った場合：学習は進むが、最小値にたどり着けず発散する可能性がある。<br>\n",
    "学習率を小さく取った場合：発散することはないが、小さすぎると収束するまでに時間がかかってしまう。<br>\n",
    "\n",
    "#### 学習率を小さく取れば良いのでは？\n",
    "学習率を小さく取ることで収束すると上記では述べたがそれでは問題がある。<br>\n",
    "**それは、収束した箇所が大域的な最小値ではなく局所的な最小点である可能性がある問題がある。**<br>\n",
    "そこで、望まない局所最小点に収束するリスクの軽減やデータが冗⻑な場合の計算コストの軽減をするため下記の手法が提案されている。<br>\n",
    "・確率的勾配降下法<br>\n",
    "・ミニバッチ勾配降下法<br>\n",
    "\n",
    "#### 確率的勾配降下法\n",
    "勾配降下法は、1回の学習に全データを使っていた。<br>\n",
    "**確率的勾配降下法**は、ランダムに抽出したサンプルごとにパラメータの更新を行う。<br>\n",
    "そのため、更新式は下記である。<br>\n",
    "確率的勾配降下法:　$w^{(t+1)} = w^{(t)} - {\\varepsilon}{\\nabla}E_n$<br><br>\n",
    "確率的勾配降下法のメリットは下記<br>\n",
    "・学習結果はサンプルのデータ順に依存するため、冗⻑なデータに対する計算コストが軽減<br>\n",
    "・望まない局所的な最小点に収束するリスクの軽減<br>\n",
    "・オンライン学習が可能<br>\n",
    "確率的勾配降下法のデメリットは下記<br>\n",
    "・例外(外れ値)データに弱い<br>\n",
    "・ランダムでデータを抽出しているので、最短で最適解にたどりつかない<br>\n",
    "#### ミニバッチ勾配降下法\n",
    "確率的勾配降下法は、ランダムに抽出したサンプルごとにパラメータの更新を行っていた。<br>\n",
    "**ミニバッチ勾配降下法**は、ランダムに分割したデータ集合(ミニバッチ)$D_t$に属するサンプルごとにパラメータの更新を行う。<br>\n",
    "そのため、更新式は下記である。<br>\n",
    "ミニバッチ勾配降下法:　$w^{(t+1)} = w^{(t)} - {\\varepsilon}{\\nabla}E_t$<br>\n",
    "　　　　　　　　　　　$E_t = {\\frac{1}{N_t}}{\\Sigma{_n}{_{\\in}}_{D{_t}}E_n}$<br>\n",
    "　　　　　　　　　　　$N_t = |D{_t}|$<br>\n",
    "           \n",
    "確率的勾配降下法のメリットを損なわず、計算機の計算資源を有効利用できる<br>\n",
    "→ CPUを利用したスレッド並列化やGPUを利用したSIMD並列化<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 誤差逆伝播法\n",
    "算出された誤差を、出力層側から順に微分し、前の層前の層へと伝播<br>\n",
    "最小限の計算で各パラメータでの微分値を解析的に計算する手法<br>\n",
    "計算結果(=誤差)から微分を逆算することで、不要な再帰的計算を避けて微分を算出できる<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 確認テスト\n",
    "#### p10:　ディープラーニングは結局何をしようとしているのか？2行以内で答えよ。また、何を最適化するのが最終目的か答えよ。\n",
    "入力値から適切な出力値を得るために，学習を行い，重みとバイアスを最適化する<br>\n",
    "→ 出力誤差(出力値と正解の誤差)を最小化するパラメータを発見することが最終目標<br>\n",
    "#### p12:　次のネットワークを紙に書け。(入力層：２ノード１層、中間層:3ノード2層、出力層:1ノード1層)\n",
    "写真を貼る。\n",
    "#### p21:　${\\omega} = {\\begin{pmatrix}{\\omega{_1}} \\\\ ： \\\\{\\omega{_I}}\\end{pmatrix}}, {x} = {\\begin{pmatrix}{x{_1}} \\\\ ： \\\\{x{_I}}\\end{pmatrix}}$の内積をpythonで記述しなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "線型結合の結果：  339.01294267299744\n",
      "線型結合の結果2：  339.01294267299744\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "W = np.random.rand(10)\n",
    "X = (np.random.rand(10)*100).astype(int)\n",
    "B = 5\n",
    "\n",
    "u = np.inner(W, X) + B #ベクトルの内積に使用する\n",
    "u2 = np.dot(W, X) + B #行列の内積に使用する\n",
    "print('線型結合の結果： ', u)\n",
    "print('線型結合の結果2： ', u2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### p26:　線形と非線形の違いを図にかいて 簡易に説明せよ。\n",
    "![線形と非線形に関するimg](写真/linear_nonlinear.JPG)\n",
    "\n",
    "#### p44:　$E_n(w) = {\\frac{1}{2}}{\\Sigma{_{i=1}^{I}}}(y_i - d_i)^2$について下記に関して答えよ。\n",
    "なぜ、引き算でなく二乗するか述べよ。さらに、下式の1/2はどういう意味を持つか述べよ。<br>\n",
    "二乗をする理由：　正の値を取るため。<br>\n",
    "1/2をする理由：　微分計算をしやすくするため。(微分すると1/2は無くなる。)<br>\n",
    "\n",
    "#### p53:　交差エントロピーのコードの処理について説明せよ。\n",
    "交差エントロピー：　　$E_n(w) = -{\\Sigma{_{i=1}^{I}}}{d_i}{\\log}{y_i}$<br>\n",
    "\n",
    "**ソースを確認していく中で、returnの式が少しわかりにくかったので簡単なものを入力して確かめた。**<br>\n",
    "ベクトルでも行列でもどちらでも対応できるようになっていることがわかった。<br>\n",
    "\n",
    "y = np.array([[0.1,0.2,0.7],[0.7,0.1,0.2],[0.1,0.7,0.2]])<br>\n",
    "y<br>\n",
    "array([[0.1, 0.2, 0.7],<br>\n",
    "       [0.7, 0.1, 0.2],<br>\n",
    "       [0.1, 0.7, 0.2]])<br>\n",
    "d = np.array([[0,0,1],[1,0,0],[0,1,0]])<br>\n",
    "d<br>\n",
    "array([[0, 0, 1],<br>\n",
    "       [1, 0, 0],<br>\n",
    "       [0, 1, 0]])<br>\n",
    "d = d.argmax(axis=1)<br>\n",
    "d<br>\n",
    "array([2, 0, 1])<br>\n",
    "batch_size = y.shape[0]<br>\n",
    "batch_size<br>\n",
    "3<br>\n",
    "-np.sum(np.log(y[np.arange(batch_size), d] + 1e-7)) / batch_size<br>\n",
    "0.3566748010815999<br>\n",
    " \n",
    "#### p53:　オンライン学習とは何か2行でまとめよ。\n",
    "データが与えられる度に逐次的に学習を行う手法<br>\n",
    "\n",
    "#### p68:　$w^{(t+1)} = w^{(t)} - {\\varepsilon}{\\nabla}E_t$の意味を図示せよ。\n",
    "![勾配効果法のimg](写真/勾配効果法について.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### p83:　① ${\\frac{\\partial{E}}{\\partial{y}}}{\\frac{\\partial{y}}{\\partial{u}}}$ と ② ${\\frac{\\partial{E}}{\\partial{y}}}{\\frac{\\partial{y}}{\\partial{u}}}{\\frac{\\partial{u}}{\\partial{\\omega{_{ji}}{^{(2)}}}}}$に該当するソースコードを探せ\n",
    "① delta2 = functions.d_mean_squared_error(d, y)<br>\n",
    "② grad['W2'] = np.dot(z1.T, delta2)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "誤差逆伝播について写経をしたので添付する。<br>\n",
    "![誤差逆伝播に関するimg](写真/誤差逆伝播.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
