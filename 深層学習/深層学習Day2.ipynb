{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 勾配消失問題\n",
    "ニューラルネットワークの火付けとなった活性化関数でシグモイド関数の紹介をした。<br>\n",
    "シグモイド関数：　$f(u) = {\\frac{1}{1+e^{-u}}}$<br><br>\n",
    "\n",
    "シグモイド関数は、確かにステップ関数では表現できなかった0~1の値を表現することを可能としていた。<br>\n",
    "しかし、層が深くなればなるほど下位層(入力層側に近い層)にはバックプロパゲーションによるフィードバックが小さくなる**勾配消失問題**が起こっていた。<br>\n",
    "\n",
    "#### なぜ勾配消失問題が起きてしまうのか？\n",
    "勾配消失問題の例として、4層のネットワークを考えてみる。(入力層１つ、中間層2つ、出力層1つ)<br>\n",
    "下位層に行くに従って、シグモイド関数の一次導関数を乗算する回数が増える例を下記に示した。<br>\n",
    "![勾配消失問題のimg](写真/勾配消失問題.JPG)<br>\n",
    "**勾配消失問題が引き起こる理由は、**<br>\n",
    "**シグモイド関数の一次導関数の取りうる値の範囲が0~0.25であること、そして層が増えるにつれて乗算回数が増えるためである。**<br>\n",
    "また、シグモイド関数の一次導関数の最大値は0.25である。(xが0の時最大値を取る。)<br><br>\n",
    "\n",
    "深層学習Day1でも書いたが、この勾配消失問題を対処するために**RELU関数**が誕生した。(スパース問題も軽減された。)<br>\n",
    "\n",
    "以後話すことではあるが、実は勾配消失問題対策でRELU関数以外にも解決策が出されていた。<br><br>\n",
    "#### 勾配消失問題対策\n",
    "・活性化関数(RELU関数)<br>\n",
    "・**初期値の設定**<br>\n",
    "・**バッチ正則化**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重みの初期化設定\n",
    "勾配消失問題は、初期値の設定によりある程度は軽減される。<br>\n",
    "今回紹介する初期化設定の手法は下記である。<br>\n",
    "・**Xavier**(ザビエルと読むらしい)<br>\n",
    "・**He**<br>\n",
    "\n",
    "#### 重みの初期化設定: Xavier\n",
    "初期値の設定方法<br>\n",
    "　重みの要素を、前の層のノード数の平方根で除算した値<br>\n",
    "\n",
    "Xavierの初期値を設定する際の活性化関数 <br>\n",
    "・ReLU関数<br>\n",
    "・シグモイド(ロジスティック)関数 <br>\n",
    "・双曲線正接関数<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Xavierによる初期化\n",
    "# network['W1'] = np.random.randn(input_layer_size, hidden_layer_size) / np.sqrt(input_layer_size)\n",
    "# network['W2'] = np.random.randn(hidden_layer_size, output_layer_size) / np.sqrt(hidden_layer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 重みの初期化設定: He\n",
    "Relu関数のための初期化設定手法。(Relu関数をさらに生かす目的で誕生した。)<br>\n",
    "Relu関数によってある程度は勾配消失は抑えられるが、*He*を用いてさらに勾配消失問題を抑える<br>\n",
    "\n",
    "初期値の設定方法<br>\n",
    "　重みの要素を、前の層のノード数の平方根で除算した値に対し$ \\sqrt{2}$ をかけ合わせた値<br>\n",
    "　Xavierの手法に対して$\\sqrt{2}$ をかけ合わせた手法になる。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Heによる初期値\n",
    "# network['W1'] = np.random.randn(input_layer_size, hidden_layer_size) / np.sqrt(input_layer_size) * np.sqrt(2)\n",
    "# network['W2'] = np.random.randn(hidden_layer_size, output_layer_size) / np.sqrt(hidden_layer_size) * np.sqrt(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# バッチ正則化\n",
    "#### バッチ正規化の効果\n",
    "・計算量の高速化<br>\n",
    "・勾配消失が起こりにくくなる<br>\n",
    "\n",
    "ミニバッチ単位で，入力値のデータの偏りを抑制する<br>\n",
    "結果として、データのばらつきが無くなる(正規化)<br>\n",
    "⇨データがコンパクトになる。<br>\n",
    "\n",
    "#### バッチ正則化のタイミング\n",
    "活性化関数に値を渡す前後に，バッチ正規化の処理を含めた層を加える<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習率最適化手法\n",
    "勾配降下法にて、学習率の話を少し触れた<br>\n",
    "復習として学習率について簡易的にまとめておく<br><br>\n",
    "**学習率の値が大きい場合**<br>\n",
    "・最適値にいつまでもたどり着かず発散する<br>\n",
    "**学習率の値が小さい場合**<br>\n",
    "・発散することはないが、小さすぎると収束するまでに時間がかかる<br>\n",
    "・大域局所最適値に収束しづらくなる<br>\n",
    "#### では、学習率はどうやって決めるのだろうか？\n",
    "初期の学習率設定方法の指針として下記であれば良いように考えられる<br>\n",
    "・初期の学習率を大きく設定し、徐々に学習率を小さくしていく<br>\n",
    "・パラメータ毎に学習率を可変させる<br>\n",
    "\n",
    "上記のようなことを実現させるために**学習率最適化手法**がある<br>\n",
    "**次から、学習率最適化手法を用いた学習率の最適化について説明する**<br>\n",
    "\n",
    "#### 学習率最適化手法は下記のような手法がある\n",
    "・モメンタム<br>\n",
    "・AdaGrad<br>\n",
    "・RMSProp<br>\n",
    "・Adam<br>\n",
    "\n",
    "#### モメンタム\n",
    "<span style=\"color: blue; \">誤差をパラメータで微分したものと学習率の積を減算した後、</span><br>\n",
    "<span style=\"color: red; \">現在の重みに前回の重みを減算した値と慣性の積を加算する</span><br>\n",
    "\n",
    "モメンタムによるパラメータ更新式は下記の②である。<br>\n",
    "$V_t = {\\mu{V_{t-1}}} - {\\varepsilon{\\nabla{E}}}$　…①<br>\n",
    "$w^{(t+1)} = w^{(t)} + V_t$　…②<br>\n",
    "${\\mu : 慣性}$<br><br>\n",
    "最初、上記の文章を見てよくわからなかったが文章通りに式変形してみると理解できた。<br>\n",
    "②の式を次に更新する$w^{(t+1)}$ではなくて、現在の重み$(w^{(t)})$について式に変形する。<br>\n",
    "すると、現在の重み$(w^{(t)})$は、下記の式になる。<br>\n",
    "$w^{(t)} = w^{(t-1)} + V_{t-1}$<br><br>\n",
    "さらに式変形すると下記になる。<br>\n",
    "$V_{t-1} = w^{(t)} - w^{(t-1)}$…③<br><br>\n",
    "この③式を①式の${V_{t-1}}$に代入をすると下記の式になる。<br>\n",
    "$V_t = {\\mu{(w^{(t)} - w^{(t-1)}})} - {\\varepsilon{\\nabla{E}}}$　…④<br><br>\n",
    "④式を②式の$V_t$代入すると下記の式になる。<br>\n",
    "$w^{(t+1)} = w^{(t)} $<span style=\"color: red; \">+ ${\\mu{(w^{(t)} - w^{(t-1)}})}$</span> <span style=\"color: blue; \">- ${\\varepsilon{\\nabla{E}}}$</span>　…⑤<br><br>\n",
    "⑤式がモメンタムの説明内容を表しており、文章と数式の色を対応づけさせた。<br>\n",
    "式変形により得られた⑤式から、**前回の重みの影響を受けている(赤色)**という意味がわかった。<br><br>\n",
    "**実装に必要な式は、①と②であることは忘れずに！**<br>\n",
    "数式とcodeは下記である。<br>\n",
    "\n",
    "<span style=\"color: green; \">################## **モメンタム** ####################</span><br>\n",
    "$V_t = {\\mu{V_{t-1}}} - {\\varepsilon{\\nabla{E}}}$<br>\n",
    "code:<br>\n",
    "$self.v[key]$ = $self.momentum * self.v[key] - self.learning$＿$rate * grad[key]$<br><br>\n",
    "$w^{(t+1)} = w^{(t)} + V_t$<br>\n",
    "code:<br>\n",
    "$params[key]$ += $self.v[key]$<br>\n",
    "<span style=\"color: green; \">######################################</span><br>\n",
    "\n",
    "モメンタムのメリット<br>\n",
    "・局所的最適解にはならず、大域的最適解となる。 <br>\n",
    "・谷間についてから最も低い位置(最適値)にいくまでの時間が早い。<br>\n",
    "\n",
    "#### AdaGrad\n",
    "誤差をパラメータで微分したものと再定義した学習率の積を減算する<br><br>\n",
    "$h_0 = {\\theta}$<br>\n",
    "$h_t = h_{t-1} + {({\\nabla{E}})^2}$<br>\n",
    "$w^{(t+1)} = w^{(t)} - {\\varepsilon {\\frac{1}{\\sqrt{h_t} + {\\theta}}} {\\nabla{E}}}$<br>\n",
    "\n",
    "<span style=\"color: green; \">###################### **AdaGrad** ####################</span><br>\n",
    "$h_0 = {\\theta}$<br>\n",
    "code:<br>\n",
    "$self.h[key] = np.zeros$_$like(val)$<br><br>\n",
    "$h_t = h_{t-1} + {({\\nabla{E}})^2}$<br>\n",
    "code:<br>\n",
    "$self.h[key]$ += $grad[key] * grad[key]$<br>\n",
    "$w^{(t+1)} = w^{(t)} - {\\varepsilon {\\frac{1}{\\sqrt{h_t} + {\\theta}}} {\\nabla{E}}}$<br>\n",
    "code:<br>\n",
    "$params[key]$ -= $self.learning$＿$rate * grad[key] / (np.sqrt(self.h[key]) + 1e-7)$<br>\n",
    "<span style=\"color: green; \">######################################</span><br>\n",
    "\n",
    "AdaGradのメリット<br>\n",
    "・勾配の緩やかな斜面に対して、最適値に近づける<br>\n",
    "課題<br>\n",
    "・学習率が徐々に小さくなるので、鞍点問題を引き起こす事があった\n",
    "\n",
    "#### RMSProp\n",
    "誤差をパラメータで微分したものと再定義した学習率の積を減算する<br><br>\n",
    "$h_t = {\\alpha{h_{t-1}}} + {(1 - {\\alpha})}{({{\\nabla}E})^2}$<br>\n",
    "$w^{(t+1)} = w^{(t)} - {\\varepsilon {\\frac{1}{\\sqrt{h_t} + {\\theta}}} {\\nabla{E}}}$<br>\n",
    "\n",
    "<span style=\"color: green; \">###################### **RMSProp** ####################</span><br>\n",
    "$h_t = {\\alpha{h_{t-1}}} + {(1 - {\\alpha})}{({{\\nabla}E})^2}$<br>\n",
    "code:<br>\n",
    "$self.h[key] *= self.decay$＿$rate$<br>\n",
    "$self.h[key] += (1 - self.decay＿rate) * grad[key] * grad[key]$<br><br>\n",
    "$w^{(t+1)} = w^{(t)} - {\\varepsilon {\\frac{1}{\\sqrt{h_t} + {\\theta}}} {\\nabla{E}}}$<br>\n",
    "code:<br>\n",
    "$params[key] -= self.learning＿rate * grad[key] / (np.sqrt(self.h[key]) + 1e-7)$<br>\n",
    "<span style=\"color: green; \">######################################</span><br>\n",
    "\n",
    "RMSPropのメリット<br>\n",
    "・(AdaGradと比較して)局所的最適解にはならず、大域的最適解となる<br>\n",
    "・ハイパーパラメータの調整が必要な場合が少ない\n",
    "#### Adam\n",
    "Adamとは，モメンタムとRMSPropの良い部分を取り入れたもの<br><br>\n",
    "モメンタムの、過去の勾配の指数関数的減衰平均<br>\n",
    "RMSPropの、過去の勾配の２乗の指数関数的減衰平均<br>\n",
    "それぞれを含んだ最適化アルゴリズムである<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 過学習\n",
    "#### 過学習とは\n",
    "テスト誤差と訓練誤差とで学習曲線が乖離すること。<br>\n",
    "特定の訓練サンプルに対して、特化して学習すること。<br>\n",
    "\n",
    "#### 過学習の原因\n",
    "・パラメータが多い<br>\n",
    "　変なところを学習してしまう可能性がある<br>\n",
    "・パラメータの値が適切でない<br>\n",
    "・ノードが多い…　など<br>\n",
    "\n",
    "⇨上記の原因らを総称して、ネットワークの自由度が高いと言う<br>\n",
    "#### 過学習の対策\n",
    "　ネットワークの自由度が高いため、過学習が引き起こされていた。<br>\n",
    "　そこでネットワークの自由度を制限し、過学習を抑制する手法ができた。<br>\n",
    "　⇨ **正則化**<br>\n",
    " \n",
    "#### 正則化\n",
    "・L1正則化、L2正則化<br>\n",
    "・ドロップアウト<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 荷重減衰\n",
    "・過学習の原因<br>\n",
    "　重みが大きい値をとることで、過学習が発生することがある<br>\n",
    "　学習させていくと、重みにばらつきが発生する。<br>\n",
    "　**重みが大きい値は、学習において重要な値であり、重みが大きいと過学習が起こる**<br><br>\n",
    "・過学習の解決策<br>\n",
    "　誤差に対して、正則化項を加算することで、重みを抑制する<br>\n",
    "　過学習がおこりそうな重みの大きさ以下で重みをコントロールし、かつ重みの大きさにばらつきを出す必要がある<br>\n",
    "\n",
    "**・L1、L2正則化**<br>\n",
    "　$E_n + {\\frac{1}{p}}{\\lambda{||x||{_p}}}$<br>\n",
    "　${||x||{_p}} = ({|x{_1}|{^p}} + ・・・ + {|x{_n}|{^p}})^{\\frac{1}{p}}$<br>\n",
    "\n",
    "　p = 1の場合、 L1正則化と呼ぶ<br>\n",
    "　p = 2の場合、 L2正則化と呼ぶ<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ドロップアウト\n",
    "正則化で最も使われている手法<br>\n",
    "　NNの学習時に、ある更新で層の中のノードのうちのいくつかを無効にして（そもそも存在しないかのように扱って）学習を行い、<br>\n",
    "　次の更新では別のノードを無効にして学習を行うことを繰り返す。<br>\n",
    "　これにより学習時にネットワークの自由度を強制的に小さくして汎化性能を上げ、過学習を避けることができる。<br>\n",
    "　⇨**データ量を変化させずに，異なるモデルを学習させていると解釈できる**<br>\n",
    "\n",
    "**問題：リッジ回帰の特徴として正しいものはどれか？**<br>\n",
    "→ ハイパーパラメータに大きな値を設定すると，すべての重みが限りなく0に近づく．<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 畳み込みニューラルネットワーク\n",
    "#### CNNとは\n",
    "画像の分野で非常によく使われている<br>\n",
    "入力の加工によっては，音声などの時系列データに対しても適用できる<br>\n",
    "複数の**畳み込み層**と**プーリング層**で構成される<br>\n",
    "#### 畳み込み層とは\n",
    "入力値にフィルター（全結合でいう重み）を掛けて，出力値を得る。それにバイアスを足して活性化関数を通す。<br>\n",
    "**フィルターは大域的な特徴を抽出するためのもの**と理解した。<br><br>\n",
    "**RGB次元の場合は？**<br>\n",
    "各チャネルに対して上記の処理と同様ことを行って最後に結果を画素ごとに全チャネルにわたって加算する<br>\n",
    "\n",
    "**CNNのメリット**<br>\n",
    "畳み込み層では，画像の場合，縦，横，チャンネルの３次元のデータをそのまま学習し，次に伝えることができる<br>\n",
    "また、チャネル間の相関関係も自動で学習(フィルタの値)してくれる優れもの。<br>\n",
    "３次元の空間情報も学習できるような層が，畳み込み層である<br>\n",
    "\n",
    "講義以外に参考にした文献も載せる<br>\n",
    "⇨http://www.orsj.or.jp/archive2/or60-4/or60_4_198.pdf<br>\n",
    "\n",
    "#### プーリング層とは\n",
    "CNNでは大域的な領域における特徴を捉えると述べた。<br>\n",
    "プーリングはCNNで捉えた特徴量の中からさらに特徴を抽出する。<br>\n",
    "プーリングには様々種類があるが今回は二つ紹介する。<br>\n",
    "\n",
    "**マックスプーリング**<br>\n",
    "　よく使われる。<br>\n",
    "　畳み込み層の出力に対してあるサイズの領域を取り出し，その最大値を出力値とする<br>\n",
    "　**CNNで捉えた特徴量の中から最も際立った特徴を抽出すると解釈しても良いと思う**<br>\n",
    "\n",
    "**アベレージプーリング**<br>\n",
    "　畳み込み層の出力に対してあるサイズの領域を取り出し，その平均値を出力値とする<br>\n",
    "\n",
    "余談だが、最近では全結合を撤廃してグローバルアベレージプーリングが主流になりつつあるらしい。<br>\n",
    "⇨パラメーター数が計算量が減り、わりと精度が出せる。<br>\n",
    "\n",
    "#### パディング\n",
    "入力画像の外側に架空の入力値を追加することで，入力画像のサイズを調整する手法。<br>\n",
    "畳み込み処理でどんどん画像が小さくなることを防ぐことができる。<br>\n",
    "さらに、パディングを入れることで畳み込み層を増やすことができる。<br>\n",
    "ゼロパディングするのが一般的。<br>\n",
    "\n",
    "**なぜゼロなのか？**<br>\n",
    "ランダムにしてしまうと値による影響を受けてしまう。<br>\n",
    "そのため、０にする。そうすることで、入力のデータの情報のまま学習ができる。<br>\n",
    "\n",
    "#### ストライド\n",
    "フィルタのずらし方のこと<br>\n",
    "8x8，パディング1の入力画像に対して，3x3のフィルタをストライド2で作用させると3x3の出力画像が得られる<br>\n",
    "6x6，パディング1の入力画像に対して2x2のフィルタをストライド1で作用させると7x7の出力画像が得られる(確認テスト参考)<br><br>\n",
    "\n",
    "スライド幅が大きいと計算量が減るが、細かい特徴を見逃す可能性がある。<br>\n",
    "逆にスライド幅が小さいと計算量が増えるが、細かい特徴を見つける可能性がある。<br>\n",
    "\n",
    "#### チャンネル\n",
    "入力画像を複数の空間に分ける（RGBなど）。分解した総数をチャンネル数という。<br>\n",
    "それぞれに対してフィルターを用意する。<br>\n",
    "\n",
    "全結合層で処理すると<br>\n",
    "縦，横，チャンネルの３次元データが１次元データとして処理される。<br>\n",
    "その結果RGBの各チャンネルの相関関係が学習に反映されない。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AlexNet\n",
    "2012年の画像認識コンペティションで２位に大差をつけて優勝，ディープラーニングが大きく注目を浴びる<br>\n",
    "**５層の畳み込み層およびプーリング層などそれに続く３層の全結合相から構成**<br>\n",
    "過学習を防ぐ施策として，サイズ4096の全結合相の出力にドロップアウトを使用<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
