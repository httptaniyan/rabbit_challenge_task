{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "### RNNとは\n",
    "時系列データに対応できる、ニューラルネットワークである。<br>\n",
    "### 時系列とは\n",
    "ある現象の時間変化を一定間隔（または不定間隔）で観測して得られた値のデータのこと。<br><br>\n",
    "具体例<br>\n",
    "⇨音声データ、株価データ、気象データ<br>\n",
    "### RNNの全体像\n",
    "<img src=\"写真/RNNの全体像3.PNG\" width=\"540\" align=\"left\">\n",
    "<img src=\"写真/RNNの全体像4.PNG\" width=\"540\" align=\"left\">\n",
    "<img src=\"写真/RNNの全体像1.PNG\" width=\"360\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNの特徴\n",
    "時系列モデルを扱うには、初期の状態と過去の時間t-1の状態を保持し、そこから次の時間でのtを再帰的に求める再帰構造が必要になる。<br>\n",
    "RNNでは中間層が重要な意味を持つ<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pythonコードを書く\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確認テスト\n",
    " RNNのネットワークには大きくわけて3つの重みがある。説明せよ。<br>\n",
    " ・入力から現在の中間層を定義する際にかけられる重み<br>\n",
    " ・中間層から出力を定義する際にかけられる重み<br>\n",
    " **・t-1の中間層からtの中間層にかけられる重み**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPTT\n",
    "### BPTTとは\n",
    "RNNにおいてのパラメータ調整方法の一種<br>\n",
    "⇨**誤差逆伝播法の一種**<br>\n",
    "#### 確認テスト(誤差逆伝播法を簡単に説明せよ)\n",
    "計算結果(=誤差)から微分を逆算することで、不要な再帰的計算を避けて微分を算出できる方法<br><br>\n",
    "\n",
    "### BPTTの全体像\n",
    "#### パラメータ更新式<br>\n",
    "#### ${\\frac{\\partial E}{\\partial W_{(in)}}}$, ${\\frac{\\partial E}{\\partial W}}$, ${\\frac{\\partial E}{\\partial W_{(out)}}}$は${\\delta}$を用いて表現可能\n",
    "<img src=\"写真/BPTT_3.PNG\" width=\"360\" align=\"left\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ${\\delta}$のまえに、、、そもそも、${\\frac{\\partial E}{\\partial W_{(in)}}}$, ${\\frac{\\partial E}{\\partial W}}$, ${\\frac{\\partial E}{\\partial W_{(out)}}}$はどうやって求めるか。\n",
    "損失関数$E$は$u^t$の関数であり、$u^t$は$W_{in}$（又は$W$）の関数である。<br>\n",
    "さらに、損失関数$E$は$v^t$の関数であり、$v^t$は$W_{out}$の関数である。<br>\n",
    "つまり合成関数で$E$を表現することが可能。<br>\n",
    "よって連鎖律によって、${\\frac{\\partial E}{\\partial W_{(in)}}}$, ${\\frac{\\partial E}{\\partial W}}$, ${\\frac{\\partial E}{\\partial W_{(out)}}}$はそれぞれを${W_{(in)}}$, $W$, ${W_{(out)}}$で偏微分することで算出することが可能。<br>\n",
    "\n",
    "<img src=\"写真/BPTT_1.PNG\" width=\"240\" align=\"left\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  そもそも、${\\delta}$ってどうやって算出した？\n",
    "${\\frac{\\partial E}{\\partial u^t}}$, ${\\frac{\\partial E}{\\partial v^t}}$をそれぞれ${\\delta ^t}$,${\\delta ^{out, t}}$に置き換えている。<br>\n",
    "${\\delta ^t}$,${\\delta ^{out, t}}$の中身の${\\frac{\\partial E}{\\partial u^t}}$, ${\\frac{\\partial E}{\\partial v^t}}$は下記のように算出される。<br>\n",
    "損失関数$E$は$v^t$の関数であり、$v^t$は$u^t$の関数である。<br>\n",
    "よって、計算も連鎖律によって算出される。<br>\n",
    "※${\\delta ^{out, t}}$の計算は省略。<br>\n",
    "<img src=\"写真/BPTT_2.PNG\" width=\"480\" align=\"left\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 損失関数の色々な表記\n",
    "下記の図を見ると、$y^t$は階層的になっていることがわかるので**$E^t$が何の関数（パラメータでも良い）かわかる**<br>\n",
    "<img src=\"写真/BPTT_4.PNG\" width=\"360\" align=\"left\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LSTM ~ 背景 ~\n",
    "### RNNの課題\n",
    "時系列を遡れば遡るほど勾配が消失してい(**勾配消失問題**)<br>\n",
    "⇨長い時系列の学習が困難<br>\n",
    "\n",
    "上記の勾配消失問題を活性化関数や正則化で解決するのではなく、<br>\n",
    "**<span style=\"color: red; \">構造自体を変えて解決したのがLSTM</span>**<br>\n",
    "\n",
    "**復習：勾配消失問題とは？**<br>\n",
    "誤差逆伝播法が下位層に伝播していくにつれて、勾配がどんどん小さくなっていく現象。(フェードバックが小さくなる)<br>\n",
    "つまり、学習が進まなくなることのことを言う。<br>\n",
    "そのため、勾配降下法による更新では、<br>\n",
    "下位層のパラメータはほとんど変わらず、訓練は最適値に収束しなくなる。<br>\n",
    "\n",
    "**数式で見る勾配消失問題の具体例**<br>\n",
    "連鎖律の計算とネットワーク図を以下に示す(左図)<br>\n",
    "下位の層に行くにつれて、活性化関数の乗算数が増えて行く様子がわかる<br>\n",
    "活性化関数にもよるが、活性化関数は1以下の値を取るため乗算する数が増えるにつれて0にどんどん近づいて行く。<br>\n",
    "そのため、パラメータの更新が進まない(学習が進まない)<br><br>\n",
    "下記のことを覚えておこう！(右図参照)<br>\n",
    "シグモイド関数の入力値が$x=0$の時、<br>\n",
    "・シグモイド関数は0.5を取る。<br>\n",
    "・シグモイド関数の１次導関数は0.25を取る。(最大値)<br>\n",
    "<img src=\"写真/勾配消失問題.JPG\" width=\"480\" align=\"left\">\n",
    "<img src=\"写真/grad_info.png\" width=\"430\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "確認テスト<br>\n",
    " シグモイド関数を微分した時、入力値が0の時に最大値をとる。その出力値は？<br>\n",
    " ⇨0.25<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 勾配爆発とは？\n",
    "勾配消失とは逆のこと<br>\n",
    "下位層に伝播して行くにつれて、勾配が指数関数的に大きくなる現象<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM ~ 全体図 ~\n",
    "LSTMの全体図が下記の図<br>\n",
    "<img src=\"写真/LSTM全体図.png\" width=\"540\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM ~ 機能分解して説明 ~\n",
    "LSTMは4つの大きな機能がある<br>\n",
    "・CEC<br>\n",
    "・入力ゲートと出力ゲート<br>\n",
    "・忘却ゲート<br>\n",
    "・覗き穴結合<br>\n",
    "\n",
    "### CEC\n",
    "RNNにおいては勾配消失および勾配爆発が課題となった<br>\n",
    "これを解決するため、勾配消失も勾配爆発も発生しない、**勾配が常に1の要素を取り入れる形とした**<br>\n",
    "<img src=\"写真/CECの式.png\" width=\"360\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CECの課題**<br>\n",
    "<span style=\"color: red; \">時系列データの意味を保持した状態で学習をすることに特化したモデルのはずなのに、<br>\n",
    "このままだと時間依存度に関係なく重みが一律であるになってしまう<br>\n",
    "⇨学習特性がなくなってしまう。</span><br>\n",
    "\n",
    "重みが一律になってしまう現象を**『重み衝突』**という<br>\n",
    "**入力重み衝突**：入力層→隠れ層への重み<br>\n",
    "**出力重み衝突**：隠れ層→出力層への重み<br>\n",
    "<img src=\"写真/CECの場所.png\" width=\"480\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 入力ゲートと出力ゲート\n",
    "#### CECの課題解決として導入された\n",
    "**入力・出力ゲートを追加することで、<br>\n",
    "それぞれのゲートへの入力値の重みを、重み行列W, Uで可変可能とする**<br>\n",
    "⇨重みの一律化の問題の改善<br><br>\n",
    "CEC自体の勾配が1であっても、入力ゲート、出力ゲートに関しては重みが存在するため、学習が可能となる。<br>\n",
    "<img src=\"写真/入力ゲート.png\" width=\"480\" align=\"left\">\n",
    "<img src=\"写真/出力ゲート.png\" width=\"460\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 忘却ゲート\n",
    "CECは、過去の情報が全て保管されている。 <br>\n",
    "過去の情報を保持することは一概に悪いことではないが、<br>\n",
    "下記の課題がある<br>\n",
    "**・過去のデータに引っ張られすぎて、すごい昔の影響も受ける**<br>\n",
    "**・一見関係性がないような状態でも、昔の影響を受ける**<br>\n",
    "\n",
    "⇨**過去の情報が要らなくなった場合、削除することはできず、保管され続ける**<br>\n",
    "\n",
    "**解決策<br>\n",
    "過去の情報が要らなくなった場合、そのタイミングで情報を忘却する機能が必要**<br>\n",
    "⇨忘却ゲートの誕生<br>\n",
    "<img src=\"写真/忘却ゲート.png\" width=\"480\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 覗き穴結合\n",
    "CEC自身の値に、重み行列を介して入力・出力ゲートや忘却ゲートに伝播可能にした構造<br>\n",
    "<img src=\"写真/覗き穴.png\" width=\"480\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "従来のLSTMでは、パラメータが多数存在していたため計算負荷が大きかった。<br>\n",
    "しかし、GRUではそのパラメータを大幅に削減し、精度は同等またはそれ以上が望める形になった構造。<br>\n",
    "あくまで、望めると言うことでタスクによってはLSTMの方が精度が高いことはある。<br>\n",
    "メリット：計算負荷が小さい<br>\n",
    "<img src=\"写真/GRU.png\" width=\"480\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**課題：LSTMとCECが抱える問題を簡潔に述べよ。**<br>\n",
    "LSTM⇨パラメータ数が多く、計算負荷が高い。<br>\n",
    "CEC⇨重みという概念がなく、勾配が1で渡され続ける。学習自体が行えない。<br>\n",
    "\n",
    "**課題： LSTMとGRUの違いを簡潔に述べよ。**<br>\n",
    "相対的に比較すると下記である。<br>\n",
    "・GRUよりも、LSTM のほうがモデルのパラメータ数が多い。<br>\n",
    "・そのため、GRUよりも、LSTM のほうが計算コストが高い。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 双方向RNN\n",
    "RNN の一種。LSTMやGRUもRNNの一種。<br>\n",
    "過去の情報だけでなく、**未来の情報を加味することで**、精度を向上させるためのモデル<br>\n",
    "双方向RNNでは、順方向と逆方向に伝播したときの中間層表現をあわせたものが特徴量となる<br>\n",
    "実際に、文章の推敲や、機械翻訳等に使用されている。<br>\n",
    "<img src=\"写真/双方向RNN.png\" width=\"480\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq\n",
    "### Seq2seqとは?\n",
    "Encoder-Decoderモデルの一種。 <br>\n",
    "実際に機械対話や、機械翻訳などに使用されている。<br>\n",
    "### Seq2seqの全体像\n",
    "<img src=\"写真/seq2seq.png\" width=\"480\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder RNN\n",
    "ユーザーがインプットしたテキストデータを、 単語等のトークンに区切って渡す構造\n",
    "<img src=\"写真/エンコーダー.png\" width=\"480\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking : 文章を単語等のトークン毎に分割し、トークンごとのIDに分割する<br>\n",
    "Embedding : IDを元にそのトークンを表す分散表現ベクトルに変換<br>\n",
    "Encoder RNN:ベクトルを順番にRNNに入力していく<br>\n",
    "\n",
    "###  Encoder RNN処理手順\n",
    "・vec1をRNNに入力し、hidden stateを出力。このhidden stateと次の入力vec2をまたRNNに入力してき たhidden stateを出力という流れを繰り返す<br>\n",
    "・最後のvecを入れたときのhidden stateをfinal stateとしてとっておく。このfinal stateがthought vectorと呼ばれ、入力した文の意味を表すベクトルとなる<br>\n",
    "\n",
    "### Decoder RNN\n",
    "Encoder RNN の逆の処理<br>\n",
    "Encoder RNNのアウトプットデータを、 単語等に生成する構造<br>\n",
    "<img src=\"写真/デコーダー.png\" width=\"480\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder RNNの処理\n",
    "1.Decoder RNN: <br>\n",
    "⇨Encoder RNNのfinal state(thought vector)から、各tokenの生成確率を出力。final stateをDecoder RNNのinitial stateとして設定し、Embeddingを入力。<br>\n",
    "2.Sampling:<br>\n",
    "⇨生成確率にもとづいてtokenをランダムに選択。<br>\n",
    "3.Embedding:<br>\n",
    "⇨2で選ばれたtokenをEmbeddingしてDecoderRNNへの次の入力。<br>\n",
    "4.Detokenize:<br>\n",
    "⇨1-3を繰り返し、2で得られたtokenを文字列に直す。<br>\n",
    "\n",
    "**確認問題**<br>\n",
    "下記の選択肢から、seq2seqについて説明しているものを選べ。<br>\n",
    "(1)時刻に関して順方向と逆方向のRNNを構成し、それら2つの中間層表現を特徴量として利用するものである。<br>\n",
    "(2)RNNを用いたEncoder-Decoderモデルの一種であり、機械翻訳などのモデルに使われる。<br>\n",
    "(3)構文木などの木構造に対して、隣接単語から表現ベクトル(フレーズ)を作るという演算を再帰的に行い(重みは共通)、文全体の表現ベクトルを得るニューラルネットワークである。<br>\n",
    "(4)RNNの一種であり、単純なRNNにおいて問題となる勾配消失問題をCECとゲートの概念を 導入することで解決したものである。<br><br>\n",
    "答え：２<br>\n",
    "\n",
    "### Seq2seqの課題\n",
    "<span style=\"color: red; \">一問一答しかできない</span><br>\n",
    "⇨問に対して文脈も何もなく、ただ応答が行われる 続ける。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HRED\n",
    "Seq2seqの課題を解決したもの<br>\n",
    "過去 n-1 個の発話から次の発話を生成する<br>\n",
    "\n",
    "<span style=\"color: red; \">Seq2seqでは、会話の文脈無視で、応答がなされたが、HREDでは、前の単語の流れに即して応答されるため、より人間らしい文章が生成される。</span>\n",
    "\n",
    "### HREDとは\n",
    "Seq2Seq+ Context RNN<br>\n",
    "Context RNN: Encoderのまとめた各文章の系列をまとめて、これまでの会話コンテキスト全体を表すベクトルに変換する構造。<br>\n",
    "⇨過去の発話の履歴を加味した返答をできる。<br>\n",
    "###  HREDの課題\n",
    "**・HREDは確率的な多様性が字面にしかなく、会話の「流れ」のような多様性が無い。**<br>\n",
    "　⇨同じ発話リストを与えられても、答えの内容が毎回会話の流れとしては同じものしか出せない。<br>\n",
    "　　同じ会話内容でもその時々で状況は異なるので同じものを返すのは不適切。<br><br>\n",
    "**・HRED は短く情報量に乏しい答えをしがち。**<br>\n",
    "　⇨短いよくある答えを学ぶ傾向がある。<br>\n",
    "　　ex)「うん」「そうだね」「・・・」など。<br>\n",
    "\n",
    "#  VHRED\n",
    "HREDの課題を解決したもの。<br>\n",
    "⇨VAEの潜在変数の概念を追加することで解決した構造。<br>\n",
    "### VHREDとは\n",
    "HREDに、VAEの潜在変数の概念を追加したもの。<br><br>\n",
    "**確認問題**<br>\n",
    "Seq2seqとHRED、HREDとVHREDの違いを簡潔に述べよ。<br>\n",
    "・Seq2seq<br>\n",
    "　⇨質問単位で学習し、対話を生成するモデル。一問一答しかできない。<br><br>\n",
    "・HRED<br>\n",
    "　⇨質問単位ではなく、これまでの会話コンテキスト全体(複数の質問)を学習し、対話を生成するモデル。Seq2seqの進化版。<br><br>\n",
    "・VHREDの違い<br>\n",
    "　⇨HREADにVAEという潜在変数の考慮を追加し、「同一コンテキストでの同一質問に毎回同じ答えを返す」という課題をある程度改善したモデル。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE\n",
    "### オートエンコーダーとは\n",
    "教師なし学習の一つ。<br>\n",
    "そのため学習時の入力データは訓練データのみで教師データは利用しない。<br>\n",
    "MNISTの場合、28x28の数字の画像を入れて、同じ画像を出力するニューラルネットワークになる。(左下図)<br>\n",
    "### オートエンコーダーの構造\n",
    "<img src=\"写真/オートエンコーダー.png\" width=\"80\" align=\"left\">\n",
    "　　オートエンコーダーは、<br>\n",
    "　　下記2つの機能を持ったネットワークである。<br><br>\n",
    "　　**Encoder**<br>\n",
    "　　⇨入力データから潜在変数zに変換する機能<br><br>\n",
    "　　**Decoder**<br>\n",
    "　　⇨潜在変数zをインプットとして元入力データを復元する機能<br><br>\n",
    "　　**オートエンコーダーのメリット**<br>\n",
    "　　次元削減が行えること。<br>\n",
    "　　※zの次元が入力データより小さい場合、次元削減とみなすことができる。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAEとは\n",
    "通常のオートエンコーダーの場合、何かしら潜在変数zにデータを押し込めているが、その構造がどのような状態かわからない。<br>\n",
    "⇨<span style=\"color: red; \">VAEはこの潜在変数zに確率分布z∼N(0,1)を仮定したもの。</span><br>\n",
    "　<span style=\"color: red; \">VAEは、データを潜在変数zの確率分布という構造に押し込めることを可能にする。</span><br>\n",
    "<img src=\"写真/VAE.png\" width=\"300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**確認問題**<br>\n",
    "VAEに関する下記の説明文中の空欄に当てはまる言葉を答えよ。<br>\n",
    "自己符号化器の潜在変数に____を導入したもの。<br>\n",
    "答え：確率分布<br>\n",
    "#  word2vec\n",
    "RNNでは、単語のような可変長の文字列をNNに与えることはできない。<br>\n",
    "⇨**固定長形式で単語を表す必要があった。**<br>\n",
    "\n",
    "可変長でも対応できるようにしたのが**word2vec**<br>\n",
    "まとめると下記になる<br>\n",
    "**・可変長の文字列を固定長の形式に変換するためもの**<br>\n",
    "・学習データからボキャブラリ（単語の集合）を作成<br>\n",
    "・ボキャブラリを次元として、入力データを one-hot-vector を作成する → 分散表現<br>\n",
    "・大規模データの分散表現の学習が、現実的な計算速度とメモリ量で実施可能となった。<br>\n",
    "　⇨従来だとボキャブラリ×ボキャブラリだけの重み行列が誕生する。<br>\n",
    " 　　<span style=\"color: red; \">**word2vec**を用いると、ボキャブラリ×任意の単語ベクトル次元で重み行列が誕生。</span><br>\n",
    "<img src=\"写真/word2vec.png\" width=\"500\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Attention Mechanism\n",
    "seq2seqは、長い文章への対応が難しいという課題があった。<br>\n",
    "seq2seqでは、2単語でも、100単語でも、固定次元ベクトルの中に入力しなければならない。<br><br>\n",
    "文章が長くなるほどそのシーケンスの内部表現の次元も大きくなっていく、仕組みが必要になる。<br>\n",
    "⇨Attention Mechanism ： 「入力と出力のどの単語が関連しているのか」の関連度を学習する仕組み。<br>\n",
    "<img src=\"写真/Attention Mechanism.png\" width=\"300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**確認問題**<br>\n",
    "RNNとword2vec、seq2seqとseq2seq+Attention Mechanismの違いを簡潔に述べよ。<br><br>\n",
    "・RNN<br>\n",
    "⇨時系列データに対するNNモデル。自然言語を対象にするとキャブラリ×ボキャブラリでデータを処理する。<br><br>\n",
    "・word2vec<br>\n",
    "⇨文章に対してボキャブラリ(単語辞書)を作成し、その単語を次元とした分散ベクトル表現で文書を表現する仕組み。ボキャブラリ×任意の単語ベクトル次元で重み行列でデータを処理する。<br><br>\n",
    "・seq2seq<br>\n",
    "⇨Encoder と Decoder を用いて、対話や機械翻訳などを行う仕組み。長い文章への対応が難しい。<br><br>\n",
    "・Attention ( Mechanism )<br>\n",
    "⇨入出力の単語の関連度合いを加味することで一種の次元削減を行い、長い文書にも対応する仕組み。seq2seqに比べ長い文章でも対応可能。<br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
