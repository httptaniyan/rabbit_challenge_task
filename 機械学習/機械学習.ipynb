{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML01_プロローグ.mp4\n",
    "### まずはじめに\n",
    "問題設定が非常に重要。<br>\n",
    "必ずしも機械学習を使う必要はない<br>\n",
    "ルールベースでできるならルールベースでも良い。<br>\n",
    "ルールーベースにするとデメリットとしては、<br>\n",
    "設定した数の分だけ設定する必要になる。数が多いと辛い。<br>\n",
    "\n",
    "また、機械学習を行うデメリットとしてはテストを行うことが難しい<br>\n",
    "通常のテストの場合は、Aに対してBが来ることを前提としているが、<br>\n",
    "機械学習はそうとは限らない。<br>\n",
    "\n",
    "### 機械学習モデリングプロセス<br>\n",
    "機械学習だけに限らず、深層学習も同じような過程を踏んでモデルを作成してゆく。<br>\n",
    "プロセスは6つある。<br>\n",
    "・問題設定<br>\n",
    "・データの選定<br>\n",
    "・データの前処理<br>\n",
    "・機械学習モデルの選定<br>\n",
    "・モデルの学習<br>\n",
    "・モデルの評価<br>\n",
    "\n",
    "#### 問題設定\n",
    "どのような課題を機械学習で解決するか<br>\n",
    "\n",
    "#### データのの選定\n",
    "問題を解決するためにどんなデータを使うか。<br>\n",
    "\n",
    "#### データの前処理\n",
    "モデルに学習させれるようにデータを変換する。<br>\n",
    "自然界やビジネスの中で得られるデータはあまり成形された状態で取得することはなかなかない。<br>\n",
    "例えば、データの欠損値などが挙げられる。<br>\n",
    "\n",
    "#### 機械学習モデルの選定\n",
    "どの機械学習モデルを利用するか<br>\n",
    "特定の問題に対してどんなモデルを使うか、<br>\n",
    "\n",
    "#### モデルの学習\n",
    "パラメータの設定の仕方。<br>\n",
    "\n",
    "#### モデルの評価\n",
    "ハイパーパラメータの選定。<br>\n",
    "モデルの精度を測る<br>\n",
    "\n",
    "大体の大枠はDLも同じ<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML02_本編\n",
    "### 機械学習とは\n",
    "・コンピュータプログラムを経験によって自動的に改善していく方法について研究<br><br>\n",
    "**・コンピュータプログラムが、タスクT(アプリケーションにさせたいこと)を性能指標Pで測定し、その性能が経験E(データ)により改善される場合、タスクTおよび性能指標Pに関して経験Eから学習す　ることと言われている。(トム・ミチェル 1997)**<br><br>\n",
    "・学習用データセット(経験E)を利用して訓練した後に、未知のデータに対して正確に予測・分類できるアルゴリズムのことを言う<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML03_線形回帰モデル\n",
    "### 回帰モデルとは\n",
    "回帰問題を解くための機械学習モデル<br>\n",
    "ある入力(数値)から出力(連続値)を予測する問題<br>\n",
    "回帰は、直線や曲線を用いて予測される。<br>\n",
    "線形回帰：データを直線で予測<br>\n",
    "非線形回帰：データを曲線で予測<br>\n",
    "\n",
    "### 回帰で扱うデータ\n",
    "入力はm次元ベクトル(m=1の場合はスカラ)<br>\n",
    "入力ベクトルの各要素は説明変数(または特徴量という)<br>\n",
    "出力はスカラー値(目的変数と呼ぶ)<br>\n",
    "\n",
    "■説明変数<br>\n",
    "　$x=(x_1, x_2, ・・・, x_m)^T {\\in} R^m$<br>\n",
    "■目的変数<br>\n",
    "　$y {\\in} R^1$<br>\n",
    " \n",
    " ### 線形回帰モデル\n",
    "　教師あり学習(正解付きデータから学習)<br>\n",
    "　入力m次元パラメータの線型結合を出力するモデル<br>\n",
    "　M次元のパラメーターとは、説明変数と同じ話<br><br>\n",
    "　**線型結合とは**<br>\n",
    "　入力と説明変数orパラメータ(いずれも特徴量)の内積をとり、切片を加えたもの<br>\n",
    "　慣例として**予測値にハット**を付ける (正解データと異なる )<br>\n",
    "  \n",
    " ■パラメータ<br>\n",
    "　パラメータ(特徴量)が予測値に対してどくらい影響を与えるかを決定する重みの集合<br>\n",
    "　${\\omega}= ({\\omega}_1, {\\omega}_2, ・・・, {\\omega}_m)^T {\\in} R^m$<br>\n",
    " ■線型結合<br>\n",
    "　$\\hat{y} = {\\omega}^{T}x + {\\omega}_0 = {\\Sigma}^{m}_{j=1}{\\omega}_{j}x_{j} + {\\omega}_0$<br>\n",
    " \n",
    " ### 線形回帰の行列表現(連立方程式を行列で表現)\n",
    "説明変数(特徴量)の集合{ $x_1$,⋯, $x_n$}と，それらに対応する目的変数の集合{$y_1$, ⋯,$y_n$} が与えられているとき，それらに対応する誤差の集合を {${\\varepsilon}_1$,⋯,${\\varepsilon}_n$} とすると，線形回帰モデルは以下のように表される<br><br>\n",
    " $y = X{\\omega} + {\\varepsilon}$ <br><br>\n",
    " ${y}= ({y}_{1}, ・・・,  {y}_{n})^T$<br>\n",
    " ${X}= ({x}_1, ・・・,  {x}_n)^T$<br>\n",
    " ${x}= (1, {x}_{i1}, ・・・,  {x}_{im})^T$<br>\n",
    " ${\\varepsilon}= ({\\varepsilon}_1, ・・・,  {\\varepsilon}_n)^T$<br>\n",
    " ${\\omega}= ({\\omega}_1, {\\omega}_2, ・・・, {\\omega}_m)^T$<br><br>\n",
    " パラメータの最適化問題は、最小二乗法により決まる。<br>\n",
    " パラメータの最適化問題とは、各パラメータの微分値が最小値になる値を探すこと。<br>\n",
    "\n",
    " # ML04_データ分割/学習\n",
    "機械学習をする際に手持ちのデータを全て学習データとして使うことはなく、<br>\n",
    "学習用データと検証用データに分けなければならない。<br>\n",
    "#### なぜ分割するか\n",
    "機械学習が本来したいことは、学習したデータに対する評価ではなく、未知のデータに対しても精度よく予測するモデルを作成することである。<br>\n",
    "つまり、モデルの**汎化性能**が高いモデルを作成することである。<br>\n",
    "当たり前のことであるが、学習したモデルに学習用データを使って評価をすると精度が高くなる。<br>\n",
    "上記のことを考慮すると、学習用データと検証用データに分ける必要があるよね。<br>\n",
    "\n",
    "#### データの分割\n",
    "学習用データ : 機械学習モデルの学習に利用するデータ<br>\n",
    "検証用データ : 学習済みモデルの精度を検証するためのデータ<br>\n",
    "\n",
    "#### 線形回帰における学習の評価指標について\n",
    "線形回帰の評価指標は、**平均二乗誤差 (残差平方和)**を用いる。<br>\n",
    "平均二乗誤差とは、データとモデルの出力の二乗誤差の和で表せすものである。<br>\n",
    "言い換えると、平均二乗誤差の結果が小さいほど直線とデータの距離が近い(似てる)と言える。<br>\n",
    "式は、下記で表すことができる。<br>\n",
    "　$MSE_{train} = \\frac{1}{n_{train}}{\\Sigma}^{n_{train}}_{i=1}({\\hat{y}^{train}_i} - {y^{train}_i} )^2$<br><br>\n",
    "検証誤差に関しては、添字のtrainがtestに変更した式になる。<br>\n",
    "#### 最小二乗法を用いて、パラメータの最適化をする\n",
    "最小二乗法とは、学習データの**平均二乗誤差**を最小とするパラメータを探索する方法である。<br>\n",
    "学習データの**平均二乗誤差の最小化**は、その勾配が0になる点を求めれば良い。<br>\n",
    "**つまり、平均二乗誤差の微分した結果(勾配)がゼロになるような${\\omega}$を探すこと。**<br>\n",
    "例としてデータがプロットされてるとすると、その点群にいい感じに直線や曲線がフィットするようなパラメータを探すことである。今回は線形回帰なので直線。<br>\n",
    " ${\\hat{\\omega}} = arg$ $min_{\\omega{\\in}{R^m+1}} {MSE_{train}}$<br>\n",
    " ⇨ $\\frac{\\partial}{{\\partial}{\\omega}}MSE_{train} = 0$<br>\n",
    "\n",
    "# ML06_線形回帰\n",
    "#### 非線形回帰\n",
    "これまで、線形回帰を取り扱って来た。<br>\n",
    "しかし、様々な現象というのは線形回帰で説明できないケースが実際多い。<br>\n",
    "大抵は、非線形で様々な現象が表現されている。<br>\n",
    "そのため、非線形回帰について学習する必要がある。<br>\n",
    "非線形回帰であっても、線形回帰と同様に考えることが可能。<br>\n",
    "具体的には、下記の３つを行う。<br>\n",
    "　1.基底関数を選択<br>\n",
    "　2.説明変数(特徴量)を選択した**基底関数**に入力(写像する)して特徴空間に飛ばす。<br>\n",
    "　　基底関数の例⇨多項式関数、ガウス型基底関数、スプライン関数 / Bスプライン関数<br>\n",
    "　3.基底関数の出力値と推定パラメータの線形結合を考える。<br>\n",
    "\n",
    "# ML07_正則化法\n",
    "機械学習や深層学習のモデルを学習をした際に、注意するべきことがある。<br>\n",
    "それは、過学習や未学習である。<br>\n",
    "#### 未学習\n",
    "　学習データとの誤差が十分小さくならない。<br>\n",
    "　言い換えると、学習データに対して学習がうまくできていない。<br>\n",
    "#### 過学習\n",
    "　学習データとの誤差は少ないが、検証データに対して誤差が大きくなる。<br>\n",
    "　言い換えると、学習データに対しては学習できているが未知のデータに対して対応できていない。(汎化性能が低い)<br>\n",
    "#### 未学習や過学習に対する対策\n",
    "　正則化法により、未学習や過学習を回避することができる。<br>\n",
    "　正則化項という考え方を導入し、回帰直線・曲線の複雑化を調整する。(曲線なめらかさを調節)<br>\n",
    " \n",
    "#### 正則化法 (罰則化法)\n",
    "モデルの複雑さに伴って、その値が大きくなる**正則化項(罰則項)を課した関数の最小化**<br>\n",
    "正則化項で利用する関数としては、例えば次のようなものがある。<br>\n",
    "L2ノルム: Ridge推定量 → パラメータ全体の総量を低く抑える。(縮小推定)<br>\n",
    "L1ノルム: Lasso推定量 → いくつかのパラメータを0に推定する。(スパース推定)<br>\n",
    "\n",
    "**制約面と損失関数の最小問題を解くこと(微分する)**になるが、イメージとしては制約面と損失関数が接する点が最小の点となることがわかっている。<br>\n",
    "具体的な計算方法は、ラグランジュの未定乗数法（KKT条件）を解くとわかるらしい。<br>\n",
    "\n",
    "# ML08_モデル選択\n",
    "#### 制約面の度合いはどのように決めればよいか\n",
    "　クロスバリデーション(交差検証)を用いて決めていく。<br>\n",
    "#### 検証の方法\n",
    "検証方法は2つある。<br>\n",
    "**・ホールドアウト法**<br>\n",
    "　訓練データを用いて学習を行い、訓練に用いていないテストデータを用いて精度検証を行う。<br>\n",
    "**・クロスバリデーション法**<br>\n",
    "　データをk個の重複しない集合に分割し、そのうちの1つをテストデータ、残りを訓練データとして訓練し、精度計算を繰り返す。<br>\n",
    "　CV値が一番大きいモデルを最終的に使用する。<br>\n",
    " \n",
    "# ML09_ロジスティック回帰\n",
    "クラス分類に使われる機械学習モデル(深層学習の活性化関数としても利用される。)<br>\n",
    "ロジスティック回帰の**入力**は $x = {\\in}{R^m}$　で、m次元である。<br>\n",
    "ロジスティック回帰の**出力**は $y = {\\in}{(0, 1)}$　で、0 or 1の2値である。<br>\n",
    "具体的には、0 or 1の結果を出力する前の処理で確率を出力。(シグモイド関数)<br>\n",
    "そして、その確率の結果から0 or 1を出力する。他クラス分類にはあまり使わない。<br>\n",
    "\n",
    "ロジスティック回帰の計算自体は、線形回帰と比較すると線型結合するところまでは同じである。**異なる部分は、線型結合後にシグモイド関数を扱うところである。**<br>\n",
    "**このシグモイド関数が線型結合を確率に変換する処理である。**<br>\n",
    "\n",
    "#### シグモイド関数<br>\n",
    "シグモイド関数　：$\\frac{1}{1+{exp^{(-ax)}}}$<br>\n",
    "**シグモイド関数の特徴**<br>\n",
    "**・入力は-∞ ~ ∞の値を取る。出力は0~1の値を取る。(確率値に変換する)**<br>\n",
    "　x = 0の時は出力が0.5。<br>\n",
    "**・aを増大させていくと勾配が急になる。**<br>\n",
    "**・シグモイド関数の微分はシグモイド自身で表現することが可能。**<br>\n",
    "　$\\frac{\\partial{\\sigma{(x)}}}{\\partial{x}} = a\\sigma(x)(1 - \\sigma(x))$<br>\n",
    "**・シグモイド関数の出力から分類結果を決める。**<br>\n",
    "　例) 0.5以下はクラス0, 0.5より大きい場合クラス１などと定義する。<br><br>\n",
    " ~を最大化する、最小化するというのは最適化問題という。<br>\n",
    " 最適化の実施には微分が必要であり、微分後の形がわかることは計算を楽にさせる。<br>\n",
    " \n",
    "# ML10_最尤推定\n",
    " 最尤推定を実施するにあたり、**尤度関数**という概念が必要になる。<br>\n",
    " 尤度関数とは、今まで取り扱っていた確率関数と考え方が逆のプロセスのものと考えると良い。<br>\n",
    " \n",
    "#### 確率関数\n",
    "確率関数は、固定されたパラメータ分布(事象が固定化されている？)から特定の確率変数がどれだけ得られやすいか表したもの。<br> \n",
    "既知の分布(確率分布)から得られやすいデータについて知るために使う。<br>\n",
    "\n",
    "#### 尤度関数\n",
    "尤度関数とは、確率分布を仮定し、あるデータからどの確率関数がどれほど尤もらしいかを表現したもの。<br>\n",
    "既知のデータから分布がどんな形か(確率分布)を知るために用いる。<br>\n",
    "\n",
    "# ML11_勾配降下法\n",
    "ロジスティック回帰の場合、尤度関数が最小となるパラメータを解析的に求めることができない。(手計算で算出がかなり困難)<br>そこで解析的に求められない場合に用いられる方法の１つとして、勾配降下法という手法がある。\n",
    "#### 勾配降下法\n",
    "勾配降下法とは、反復計算（勾配降下）によって(逐次的に)パラメータを更新する手法である。<br>\n",
    "デメリットとしては、１回のパラメータ更新のために、すべての入力データが必要であることがあげられる。<br>\n",
    "入力データが膨大になると、計算時間・記憶容量が問題となる。<br>\n",
    "この点を解決するために、確率的勾配降下法というものが存在する。<br>\n",
    "#### 確率的勾配降下法\n",
    "データを一つずつランダムに選んでパラメータを更新する。<br>\n",
    "勾配降下法でパラメータを1回更新するのと同じ計算量でパラメータを n回更新できる。<br>\n",
    "確率的勾配法は、勾配降下法に比べ大域的最適解を見つけやすい。<br>\n",
    "言い換えると、勾配降下法に比べ、局所最適化にハマりにくい。<br>\n",
    "#### ミニバッチ勾配降下法\n",
    "勾配降下法と確率的勾配法の良いところを合わせたようなもの。<br>\n",
    "全データをn個の塊に分ける。<br>\n",
    "そして、分けた塊をランダムに選んで、塊ごとに学習をさせる方法。<br>\n",
    "塊をn=1にすると確率的勾配降下法と同じ意味になる。<br>\n",
    "\n",
    "# ML12_確率的勾配法\n",
    "#### デモ動画を見て。。。\n",
    "パラメータに初期値を与え、徐々にパラメータを更新し、収束した時点でそのときのパラメータを最適値として採用する。<br>\n",
    "学習率ηは、学習の収束のしやすさを表す。パラメータ更新の歩幅が大きくなるイメージ。<br>\n",
    "ηが小さいと収束するまでに時間がかかる。<br>\n",
    "しかし、ηを大きくしてしまうと、大域的局所解を飛び越えるのが難しくなる。<br>\n",
    "逆に小さすぎるのもよろしくない。<br>\n",
    "\n",
    "# ML13_モデルの評価\n",
    "#### 混同行列\n",
    "・予測値とデータの結果が合致<br>\n",
    "　- 予測が Positive: True Positive (TP)<br>\n",
    "　- 予測が Negative: True Negative (TN)<br>\n",
    "・予測値とデータの結果が合致しない<br>\n",
    "　- 予測が Positive: False Positive (FP)<br>\n",
    "　- 予測が Negative: False NegativFN)<br>\n",
    "#### 正解率、適合率、再現率、F値\n",
    "正解率：$\\frac{TP}{TP + TN + FP + FN}$<br>\n",
    "再現率：$\\frac{TP}{TP + FN}$・・・Recallとも呼ばれる<br>\n",
    "適合率：$\\frac{TP}{TP + FP}$・・・Precisionとも呼ばれる<br>\n",
    "F値　　:2×$\\frac{適合率×再現率}{適合率 + 再現率}$<br>\n",
    "　　　F値はRecallとPrecisionの調和平均<br>\n",
    "ケースによって、再現率や適合率どちらを重視するか変わる。<br><br>\n",
    "RecallとPrecisionはトレードオフの関係なため、<br>\n",
    "Recallを維持したままPrecisionを上げる、Precisionを保ったままRecallを上げることは難しい。<br>\n",
    "\n",
    "# ML17_主成分分析\n",
    "**主成分分析とは、<br>\n",
    "多次元データが持つ構造をより少数の指標にまとめる手法**<br><br>\n",
    "主に下記に使用される。<br>\n",
    "・次元圧縮<br>\n",
    "・少数変数にまとめることによって可視化が可能（100次元→2,3次元）<br><br>\n",
    "\n",
    "**どうやって多次元データが持つ構造を捉えるか？**<br>\n",
    "学習データの分散が最大になる方向への線形変換を求めれば良い。<br>\n",
    "主成分分析では、分散の大きさを情報量の大きさとして考えている。<br>\n",
    "そのため、分散が大きい方向への一次変換を考えている。<br><br>\n",
    "**【再喝】まとめると。。。<br>\n",
    "　　多次元データの情報(構造又は特徴)を、\n",
    "<br>\n",
    "　　出来るだけ損失せずに低次元で表現する手法**\n",
    "<br><br>\n",
    "情報量の大きさ＝分散の大きさと考えること。<br>\n",
    "なぜ、分散が情報量の大きさを表すかは後に後述する。<br>\n",
    "\n",
    "### 数式に落とし込む\n",
    "学習データ<br>\n",
    "　$x = (x_1, x_2, ・・・, x_m)^T {\\in} R^m$\n",
    "\n",
    "平均ベクトル<br>\n",
    "　$\\bar{x} = {\\frac{1}{n}}{{\\Sigma}^{n}_{i=1}}x_i$\n",
    "\n",
    "データ行列を平均ベクトルとの差分から作成<br>\n",
    "　$X = (x_1 - {\\bar{x_1}}, x_2 - {\\bar{x_2}}, ・・・, x_m - {\\bar{x_m}})^T$\n",
    "\n",
    "分散共分散行列<br>\n",
    "　${\\Sigma} = Var({\\bar{X}}) = {\\frac{1}{n}}{\\bar{X}^T}{\\bar{X}}$\n",
    "\n",
    "n個のデータを係数ベクトルを用いて線形変換<br>\n",
    "　$s_j = (s_{1j}, s_{2j}, ・・・, s_{nj})^T = {\\bar{X}}a_j$\n",
    "\n",
    "\n",
    "**線形変換後の分散が最大となる射影軸（線形変換）を探索する**<br>\n",
    "線形変換後の分散は、<br>\n",
    "下記の通りでこれを**最大化する係数ベクトル$a_j$**を求める<br>\n",
    "　$Var(s_j) = {\\frac{1}{n}}{s^T_j}{s_j}$<br>\n",
    " 　　　　　$= {\\frac{1}{n}}({{\\bar{X}}a_j})^T{{\\bar{X}}a_j}$<br>\n",
    " 　　　　　$= {\\frac{1}{n}}{a_j^T{\\bar{X}}^T}{{\\bar{X}}a_j}$<br>\n",
    " 　　　　　$= {\\frac{1}{n}}{a_j^T}{\\Sigma}{a_j}$<br>\n",
    " 　　　　　$= {\\frac{1}{n}}{a_j^T} Var({\\bar{X}}){a_j}$<br><br>\n",
    "以下の最適化問題を解く (**ノルムに制約を入れないと無限に解がある**)<br>\n",
    "　$arg$ $max_{a{\\in}{R^m}} {a_j^T} Var({\\bar{X}}){a_j}$<br>\n",
    "　subject to　$a^T_j$$a_j$$ = 1$　⇦　ノルムの制限<br>\n",
    " \n",
    "上記の制約付き最適化問題はラグランジュ関数を最大にする係数ベクトルを見つければいい<br>\n",
    "　・係数ベクトルで微分して0と置き解を求める<br>\n",
    "　・分散共分散行列の固有ベクトルが解となる　→　固有値問題に帰着する<br>\n",
    " \n",
    "**ラグランジュ関数(⇨参考サイト：https://mathtrain.jp/mlm)**<br>\n",
    "　$E(a_j) = {a_j^T} Var({\\bar{X}}){a_j} - {\\lambda}{(a^T_ja_j - 1)}$<br>\n",
    "　$\\frac{\\partial{E(a_j)}}{\\partial{a_j}} = 2Var({\\bar{X}}){a_j} - 2{\\lambda}a_j = 0　ー※$<br>\n",
    "　$Var({\\bar{X}}){a_j} = {\\lambda}a_j$　⇦ **分散共分散行列の固有値問題を解く**<br><br>\n",
    "上記の式からわかることが下記<br>\n",
    "・射影ベクトル($Var({\\bar{X}}){a_j}$)の向きは固有ベクトル<br>\n",
    "・分散の値は固有値と一致<br>\n",
    "　　$Var(s_1) = a^T_jVar({\\bar{X}}){a_j} = {\\lambda}_1a^T_ja_j = {\\lambda}_1$<br><br>\n",
    "よって、<br>\n",
    "**線形変換後の分散は、分散共分散行列の固有値と一致することを証明した。**<br>\n",
    "分散共分散行列は実対象行列なので，固有ベクトルはすべて直行する。<br><br>\n",
    "**主成分**<br>\n",
    "　最大固有値に対応する固有ベクトルで線形変換された特徴量を第1主成分と呼ぶ<br>\n",
    "　k番目の固有値に対応する固有ベクトルで変換された特徴量を第k主成分と呼ぶ<br><br>\n",
    "**寄与率**<br>\n",
    "　k番目の分散の全分散に対する割合を第k主成分の寄与率という<br>\n",
    "\n",
    "※行列が対称行列で、横ベクトルと縦ベクトルの成分が同じであるようなものを二次形式と呼ぶ。この二次形式の微分については下記のサイトを参照した。<br>\n",
    "　https://mathtrain.jp/quadraticform\n",
    " \n",
    "余談ではあるが<br>\n",
    "対称行列Aがある場合、下記の性質がある<br>\n",
    "$A^T = A$<br><br>\n",
    "例題：\n",
    "$(A^TA)^T = A^TA$<br>\n",
    "\n",
    "# ML18_K近傍法/K平均法\n",
    "### k近傍法\n",
    "**k近傍法とは、データの近傍周辺におけるデータを見て、多数決によりクラスを決定する手法**<br>\n",
    "分類のために使われる手法である。<br><br>\n",
    "例えば：<br>\n",
    "ある点の近傍周辺にクラス1が2個、クラス2が1個、クラス3が3個となっている場合。<br>\n",
    "そのときある点のクラスは3と決定する。<br>\n",
    "あらかじめ、kは決定する必要がある。<br>\n",
    "\n",
    "k近傍法のkは、あるデータ近傍周辺で何個のデータを見るかという意味のk。<br>\n",
    "上記の例ではkがk=6である。<br>\n",
    "k=1の時の呼び名は、最近傍法である。<br>\n",
    "\n",
    "・kが小さい場合は、近傍の点に大きく左右されてしまうので、バリアンスが高いような状態となる。<br>\n",
    "・kを大きくすると境界線はなめらかになる。<br>\n",
    "\n",
    "### k平均法(k-means)\n",
    "**k平均法とは、与えられたデータをk個のクラスタに分類する手法である**<br>\n",
    "k平均法は教師なしで、クラスタリングをする手法として分類される。<br>\n",
    "Kは事前に決める必要がある。<br>\n",
    "クラスタリングとは、特徴が似ているもの同士をグループ化してくれる手法。<br>\n",
    "\n",
    "中心の初期値を変えるとクラスタリングの結果も変わりうる。<br>\n",
    "そのため、何回か初期値を変えて結果が変わらないことを確認する必要がある。<br>\n",
    "\n",
    "Kの値を変えるとクラスタリング結果も変わる。<br>\n",
    "Kの数を決め方は難しい。大体はナレッジ次第らしい。。。<br>\n",
    "\n",
    "#### k平均法の計算手順\n",
    "(1) 各クラスタ中心の初期値を設定する <br>\n",
    "(2) 各データ点に対して、各クラスタ中心との距離を計算し、 最も距離が近いクラスタを割り当てる <br>\n",
    "(3) 各クラスタの平均ベクトル(中心)を計算する <br>\n",
    "(4) 収束するまで2, 3の処理を繰り返す <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
