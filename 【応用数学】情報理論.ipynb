{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # MA17_情報理論1-1\n",
    " **情報量とは？**<br><br>\n",
    "情報の量。<br>\n",
    "珍しい情報ほど価値がある。<br>\n",
    "珍しさの測り方は、何回に一回出るか？などと確率で表現する。<br>\n",
    "\n",
    "**自己情報量**<br><br>\n",
    "　$I(x)=−log(P(x))$\n",
    "\n",
    "珍しいことが起こる(確率が小さい)と$I(x)$は小さくなることを示している。<br>\n",
    "しかし、**珍しい情報ほど価値がある**のが**『情報量』**なのに値が小さくなっている。<br>\n",
    "だから、逆数で表した表現(Wで表す)で**『情報量』**を表すこともある。<br>\n",
    "それが下記の式。<br>\n",
    "　$I(x) = −log(P(x)) = log(W(x))$<br>\n",
    "\n",
    "#### なぜ、情報量はlogを用いて表現されるか？\n",
    "人間は何か量が増えた時、足し算で増えると認識する。<br>\n",
    "しかし、同時確率等を考えると、事象の積であらわされてしまう。<br>\n",
    "これを**加算であらわす形に変換するためにlogを用いている。**<br>\n",
    "**その他に、確率を用いているのでlogを使うことで珍しい事象(確率が小さい)を大きい値として表現できる。**<br>\n",
    "底を2とした場合の単位は bit<br>\n",
    "底を e とした場合の単位は nat<br>\n",
    "\n",
    "# MA18_情報理論1-2\n",
    "#### 人間は、どうやって情報量が増えたことを認識しているのか？\n",
    "元々の事象の量に対して新たに増えた事象の量の比を、<br>\n",
    "どうやら増えた情報と認識しているようだ。<br>\n",
    "式で表すと下記である。<br>\n",
    "　${\\Delta}I$ = $\\frac{{\\Delta}W}{W}$<br><br>\n",
    " 積分すると...<br>\n",
    " 　$I$ = $log_e{W}$で表現される。<br>\n",
    "  このことから、人間は人間は情報の増え方を対数で見ていると考えられる。<br>\n",
    "#### シャノンエントロピー(平均情報量 or 自己情報量の期待値)\n",
    "自己情報量の期待値として表現される。<br>\n",
    " $H(x)=E(I(x))$<br>\n",
    " $=−E(log(P(x)))=−∑(P(x)log(P(x)))$\n",
    "\n",
    "# MA19_情報理論2\n",
    "自己情報量により、ある事件の自体の情報量を見積れる。<br>\n",
    "シャノンエントロピーにより、そういった事件が様々な確率で出現されるときに平均的にどれくらい期待できるかを見積れるようになった。<br>\n",
    "確率分布が違った場合、それって情報量にどのくらい違いがあるのだろうか？<br>\n",
    "\n",
    "#### カルバック・ライブラー ダイバージェンス\n",
    "2つの確率分布の違いによる情報量の大きさの違いを計算するために使用される。\n",
    "情報量の大きさは確率分布によって決まり、確率分布が違えば、全体の情報量の大きさが変わるということを示している式である。<br>\n",
    "式は下記である。<br><br>\n",
    "　$D_{KL}$$(P||Q)$ = $E_{x〜P　}[{log}\\frac{P(X)}{Q(X)}]$ = $E_{x〜P　}[{logP(X)}{-}{logQ(X)}]$<br><br>\n",
    " 新しい確率分布 P(x) を重みとして平均をとることがポイント。<br><br>\n",
    " ※PとQを逆にすると値が異なるため、厳密には距離ではない。<br>\n",
    " \n",
    " # MA19_情報理論3\n",
    "#### 交差エントロピー\n",
    "　$H(P, Q)$ = $H(P)$ + $D_{KL}$$(P||Q)$<br>\n",
    "　$H(P, Q)$ = $-$ $E_{x〜P　}{logQ(x)}$<br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
